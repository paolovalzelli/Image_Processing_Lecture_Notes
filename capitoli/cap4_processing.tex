\chapter{Digital Image Processing}
\label{ch:processing}

The \textbf{digital image chain} comprises an acquisition device, an input/output storage system, a computer for processing, and a visualization device.
\textbf{Digital image processing} involves the use of computer algorithms to manipulate digital images.
The two primary advantages over analogue processing are the broader range of applicable algorithms and the reproducibility of the processes.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\linewidth]{immagini/flowchart_steps.pdf}
    \caption{Fundamental steps in image analysis.}
    \label{fig:steps}
\end{figure}

\section{Image Acquisition}

The acquisition device is a detector composed of millions of photosensitive sites (pixels). 
Here, radiation is converted into an electrical charge proportional to the intensity, which is subsequently converted into a digital number.

\subsection{Scintillation devices and semiconductor detectors}

A \textbf{scintillator} is a material that emits visible light when excited by incoming radiation. 
It can be coupled to an electronic light sensor to form a scintillation detector. 
This process achieves \textit{indirect conversion}: incoming ionising radiation is first converted into visible light, which is then converted into an electric charge.

In a \textbf{semiconductor detector} (or solid-state detector), a sensitive semiconductor layer produces charge carriers (electrons and holes) directly when excited by incoming radiation. 
These charges are collected via an electric field, achieving \textit{direct conversion}.

In scintillators, light produced by the Compton or photoelectric effect is emitted isotropically, creating a cone of photons. 
This results in a "spot" for every interaction, potentially blurring the image. 
Reducing the layer thickness limits the spread but reduces detection efficiency.
To mitigate this, structured scintillators using thin ($\approx 20 \mu$m) needle-shaped columns act as light guides, reducing spread without excessively compromising efficiency.
In solid-state detectors, direct conversion significantly minimises signal spread, generally yielding images of superior sharpness.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{immagini/scintillator.png}
    \caption{Direct and indirect light conversion by detectors.}
    \label{fig:scintillator}
\end{figure}

\section{Digitiser Characteristics}

Key characteristics of the digitisation system include:

\begin{itemize}
    \item \textbf{pixel size:} It directly affects spatial resolution. Physical limits are imposed by the electronic components, and reducing pixel size can significantly increase costs.
    \item \textbf{image size (FOV):} While a larger image area is generally desirable, the size can be constrained by the manufacturing costs of the digitiser \textcolor{gray}{(e.g., silicon wafer size)}.
    \item \textbf{linearity:} Ideally, the device response function should be linear. In some cases, the relationship between input $I$ and output $O$ requires correction via a calibration curve:
    
    \begin{equation}
        O = \text{gain} \cdot I^\gamma + \text{offset}
    \end{equation}
    
    \item \textbf{noise:} It is introduced by the detector itself. While statistical noise is inherent to the signal, electronic noise introduced by the digitiser degrades image quality.
\end{itemize}

\textbf{Flat panel detectors (FPD)} are compact devices that use different kinds of sensitive layers.
Most photodiodes are composed of arrays of \textbf{thin film transistors (TFTs)}. 
Each pixel cell contains a transistor switch for readout and a capacitor to collect the charge.

\section{Image Visualisation}

A display system reconstructs an image as a distribution of visible light based on the digital values. 
This conversion inevitably introduces distortions that should be minimised.

\begin{itemize}
    \item \textbf{Soft copy:} a temporary image displayed on a monitor (volatile display).
    \item \textbf{Hard copy:} a permanent printed image (permanent display).
\end{itemize}

\subsection{Visualisation device features}

\begin{itemize}
    \item \textbf{Displayed image size:} the capability to correctly display large matrices. Ideally, the monitor should have enough pixels to display the image without downscaling; otherwise, zooming is required.
    \item \textbf{Photometric resolution:} the accuracy of displayed intensity at each pixel. It depends on the number of discrete grey levels and electronic noise. If noise fluctuations exceed the step between grey levels, the effective resolution is reduced.
    \item \textbf{Response curve:} the transfer function from input grey level to output brightness (often related to Gamma). Ideally, it should be linear and stable over time.
    \item \textbf{Calibration:} the procedure of measuring the device response and adjusting parameters to ensure correct rendering. A phantom image is often used to test the response.
    \item \textbf{Luminance:} the brightness of the display. High luminance is desirable, especially in medical diagnostics, to distinguish subtle grey-level variations.
    \item \textbf{Reliability:} the stability of device characteristics over time. Periodic calibration is required to correct variations in the transfer function.
\end{itemize}

\section{Image Formats}

Image formats are characterised by:

\begin{itemize}
    \item \textbf{type} (raster or vector);
    \item \textbf{compression};
    \item \textbf{bit depth};
    \item \textbf{header information}.
\end{itemize}

\subsection{Raster vs. vector}

\textbf{Raster formats} store images as a matrix of numbers representing the grey levels of pixels.

\begin{itemize}
    \item Examples: \texttt{.jpg}, \texttt{.gif}, \texttt{.tiff}, \texttt{.raw}, \texttt{.bmp}, \texttt{.dcm} (DICOM).
\end{itemize}

\textbf{Vector formats} describe shapes analytically using mathematical definitions. 
They are efficient for images with simple geometric shapes or text but unsuitable for photographic data.

\begin{itemize}
    \item Examples: \texttt{.ps}, \texttt{.eps}, \texttt{.pdf}.
\end{itemize}

\subsection{Header}

Most formats include a \textbf{header} containing metadata before the actual image data.
\textbf{Raw} files often lack a standard header, containing only the matrix data. 
Other formats store information such as image dimensions (rows, columns), bit depth, and acquisition date in the header.

\section{Geometric Operations}

\textbf{Geometric transformations} modify the spatial relationship of pixels (position, shape) without ideally changing their grey level. 
They involve two steps:

\begin{enumerate}
    \item \textbf{Spatial mapping:} determining the new coordinates of each pixel.
    \item \textbf{Grey-level interpolation:} assigning brightness values to the new pixels.
\end{enumerate}

\subsection{Spatial transformation}

A spatial transformation maps the original image $f_0$ to a transformed image $f_t$:

\begin{equation}
    f_t (x', y') = f_0 (x,y) \quad \text{where} \quad x' = t_1 (x,y), \quad y' = t_2(x,y)
\end{equation}

The most common class is \textbf{affine transformations} (rigid body transformations), which preserve parallelism. 
They are applied using matrix multiplication on homogeneous coordinates:

\begin{equation}
    \begin{bmatrix}
        x' \\ y' \\ 1
    \end{bmatrix}
    =
    \begin{bmatrix}
        a_{00} & a_{01} & a_{02} \\
        a_{10} & a_{11} & a_{12} \\
        a_{20} & a_{21} & a_{22}
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
        x \\ y \\ 1
    \end{bmatrix}
\end{equation}

Common affine transformations include:

\begin{itemize}
    \item \textbf{translation} (shift by $\Delta x, \Delta y$):
    \begin{equation}
    \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} =
    \begin{bmatrix} 1 & 0 & \Delta x \\ 0 & 1 & \Delta y \\ 0 & 0 & 1 \end{bmatrix}
    \cdot
    \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}
    \end{equation}

    \item \textbf{scaling} (resize by factors $s_x, s_y$):
    \begin{equation}
    \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} =
    \begin{bmatrix} s_x & 0 & 0 \\ 0 & s_y & 0 \\ 0 & 0 & 1 \end{bmatrix}
    \cdot
    \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}
    \end{equation}

    \item \textbf{rotation} (rotate by angle $\theta$):
    \begin{equation}
    \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} =
    \begin{bmatrix} \cos \theta & -\sin \theta & 0 \\ \sin \theta & \cos \theta & 0 \\ 0 & 0 & 1 \end{bmatrix}
    \cdot
    \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}
    \end{equation}

    \item \textbf{reflection} (flip along axes):
    \[
    \text{X-axis:} \begin{bmatrix} -1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}, \quad
    \text{Y-axis:} \begin{bmatrix} 1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
    \]
\end{itemize}

\subsection{Grey level interpolation}

Since mapped coordinates $(x,y)$ may not be integers, interpolation is needed to assign a value to the output pixel.

\begin{itemize}
    \item \textbf{Nearest neighbour:} assigns the value of the closest grid point without interpolation. Fast, but produces jagged edges (aliasing), so that in some cases smoothing is applied.
    \item \textbf{Bilinear interpolation:} weighted average of the 4 (or, in some cases, 9) closest pixels. Smoother, but can blur edges.
\end{itemize}

The choice depends on the application requirements (speed vs. quality).

\subsection{Applications}

Geometric operations are used for:

\begin{itemize}
    \item \textbf{correction}: fixing geometric distortions from the acquisition device (calibration step).
    \item \textbf{registration}: aligning multiple images of the same scene (e.g., multispectral).
    \item \textbf{image fusion}: combining registered images from different modalities (e.g., in nuclear medicine) to provide complementary information.
\end{itemize}

\section{Grey Level Operations}

\textbf{Grey level operations} manipulate pixel intensity. 
They are classified by the spatial scope of the input:

\begin{itemize}
    \item \textbf{Point operations:} Output at $(x,y)$ depends only on input at $(x,y)$.
    \item \textbf{Local operations:} Output depends on a neighbourhood of $(x,y)$.
    \item \textbf{Global operations:} Output depends on all pixels in the image.
\end{itemize}

Complexity is measured in \textit{operations per pixel}.

\subsection{Image histogram}

The \textbf{image histogram} shows the distribution of grey levels, discarding spatial information. 
It helps assess contrast and exposure.
It is defined as a discrete function where $n_k$ is the count of pixels with grey level $r_k$:

\begin{equation}
    p(r_k) = \frac{n_k}{N} \quad \text{for } k = 0, 1, \dots, L-1
\end{equation}

where $N$ is the total number of pixels. 
A spread around a single value in the histogram of a uniform object indicates noise.

\subsection{Thresholding}

\textbf{Thresholding} is a point operation that creates a binary image. 
Pixels above a threshold $T$ become white, others black:

\begin{equation}
    g(x,y) = \begin{cases}
        1 & \text{if } f(x,y) > T \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}

This is a fundamental step in segmentation for separating the foreground from the background and reducing noise.