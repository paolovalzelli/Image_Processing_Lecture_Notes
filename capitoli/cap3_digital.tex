\chapter{Introduction to Digital Images}
\label{ch:digital_images}

\section{Image Formation}

When a 3D object is mapped onto a 2D image plane, geometric relationships govern the location and size of the representations, while radiometric relationships determine the intensity of light radiating from the object and impinging on the corresponding image point.

\subsection{Pinhole camera}

The earliest camera models were boxes with a small aperture, known as the \textit{pinhole}, which began to appear during the 16th century. 
The size of the resulting image depends on both the object's size and its distance from the camera.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{immagini/pin-hole_camera.png}
    \caption{Pinhole camera geometry.}
    \label{fig:pin-hole_camera}
\end{figure}

As observed in Figure \ref{fig:pin-hole_camera}, distant objects appear smaller according to the similarity of triangles:

\begin{equation}
    \frac{y}{d} = \frac{h}{f}
\end{equation}

where $y$ is the object size, $d$ is its distance from the pinhole, $h$ is the corresponding image size, and $f$ is the distance from the pinhole to the image plane, known as the \textit{focal distance}. 
The point $O$ represents the \textit{optical centre}.

If $(u,v)$ describes the coordinate system of the image plane and the acquisition device is characterised by the system $(u,v,z)$, where $z$ is the \textit{optical axis} (perpendicular to the image plane and passing through the optical centre), we can map a point $M(x,y,z)$ in object space to a point $m(u,v)$ in the image plane:

\begin{equation}
    M(x,y,z) \longrightarrow m(u,v) \quad \text{where} \quad u = f \frac{x}{z}, \quad v = f \frac{y}{z}
\end{equation}

\subsection{Orthographic projection}

A useful approximation, known as \textbf{orthographic projection}, applies when the size of the object is significantly smaller than its distance from the optical centre. 
In this case, light rays are considered parallel to the optical axis and orthogonal to the image plane.
If the object depth is $2 \Delta z$ and its average distance is $z_0$, with $\Delta z \ll z_0$:

\begin{equation}
    \frac{f}{z_0 + \Delta z} \approx \frac{f}{z_0 - \Delta z} \approx \frac{f}{z_0} \implies u \approx \frac{f}{z_0} x, \quad v \approx \frac{f}{z_0} y
\end{equation}

This implies that magnification is essentially constant across the object.

\subsection{Pinhole size}

The diameter of the pinhole presents a trade-off. 
A smaller pinhole produces sharper images but drastically reduces the light flux reaching the sensor, requiring longer exposure times (which can be problematic for moving objects). 
Conversely, a larger pinhole introduces blurring \textcolor{gray}{(geometric unsharpness)}, as each point of the object projects onto a finite spot \textcolor{gray}{(circle of confusion)} on the image plane.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{immagini/pin-hole_size.png}
    \caption{Effects of pinhole size on the image.}
    \label{fig:pin-hole_size}
\end{figure}

\subsection{Cameras with lenses}

Cameras equipped with lenses gather significantly more light and focus it onto a single point on the image plane, allowing for reduced exposure times.
The \textit{thin lens model} provides a useful approximation for simple optical systems.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{immagini/thin_lens.png}
    \caption{Thin lens model.}
    \label{fig:thin_lens}
\end{figure}

Let $u$ be the object distance from the optical centre, $v$ the image distance, and $f$ the focal length. The thin lens equation relates these quantities:

\begin{equation}
    \frac{1}{u} + \frac{1}{v} = \frac{1}{f}
\end{equation}

For rays coming from infinity ($u \to \infty$), the image forms at the focal plane ($v = f$).

\subsection{Geometry of imaging devices}

Most acquisition devices consist of a combination of lenses that concentrate light onto an image plane where sensors convert the light into an electrical signal.

Since 3D points project onto a 2D plane, depth information is inherently lost. 
Depth can be recovered using techniques such as \textbf{stereoscopy} (using two views that mimick human binocular vision) or \textbf{tomography} (using thousands of projections, which is typical in medical imaging).

\subsection{Field Of View (FOV)}

The \textbf{field of View} (FOV) is the angular extent of the scene captured by the camera. 
It depends on the focal length $f$ and the sensor dimensions $(w, h)$.

\begin{equation}
    \text{FOV}_{\text{horiz}} = 2 \arctan \left( \frac{w}{2f}\right), \quad \text{FOV}_{\text{vert}} = 2 \arctan \left( \frac{h}{2f}\right)
\end{equation}

Increasing the focal length $f$ reduces the field of view \textcolor{gray}{(telephoto)}, while decreasing $f$ increases it \textcolor{gray}{(wide-angle)}.

\paragraph{Example}

Consider a sensor with effective area $w \times h = \SI{8.8}{\milli\meter} \times \SI{6.6}{\milli\meter}$.

\begin{itemize}
    \item For $f_1 = \SI{12}{\milli\meter}$:
    \[ \text{FOV}_{\text{horiz}} = 2 \arctan \left( \frac{8.8}{2 \cdot 12}\right) \approx \ang{40}, \quad \text{FOV}_{\text{vert}} \approx \ang{30} \]
    \item For $f_2 = \SI{50}{\milli\meter}$:
    \[ \text{FOV}_{\text{horiz}} = 2 \arctan \left( \frac{8.8}{2 \cdot 50}\right) \approx \ang{10}, \quad \text{FOV}_{\text{vert}} \approx \ang{7.5} \]
\end{itemize}

\subsection{Magnification factor}

The \textbf{magnification factor} $M$ is the ratio of the image size $x$ to the object size $X$:

\begin{equation}
    M = \frac{x}{X} = \frac{v}{u} \approx \frac{f}{u} \quad (\text{if } u \gg f)
\end{equation}

$M$ is proportional to the focal length. 
If $M > 1$, the image is larger than the object (magnification).

\paragraph{Example}

An object of width $X = \SI{0.5}{\meter}$ is placed at a distance $u = \SI{3}{\meter}$.

\begin{itemize}
    \item For $f_1 = \SI{12}{\milli\meter}$: $x = \left( \dfrac{12}{3000} \right) \cdot 500 = \SI{2}{\milli\meter}$
    \item For $f_2 = \SI{50}{\milli\meter}$: $x = \left( \dfrac{50}{3000} \right) \cdot 500 = \SI{8.33}{\milli\meter}$
\end{itemize}

Thus, a small focal length yields a large FOV and small magnification, while a large focal length yields a small FOV and large magnification.

\subsection{Optical distortions}

Real optical systems introduce distortions, particularly far from the optical axis. 
A common type is \textbf{radial distortion} \textcolor{gray}{(e.g., barrel distortion)}, that is prominent in fish-eye lenses.

\subsection{Lens radiometry}

Ideally, the radiance reaching the image matches the object radiance. 
However, lens geometry affects irradiance.
Let $d$ be the lens diameter, $f$ the focal length, and $\alpha$ the angle of the light ray with the optical axis. 
The irradiance $E(p)$ at image point $p$ relates to object exitance $L(P)$ by the \textbf{cosine fourth law}:

\begin{equation}
    E(p) = L(P) \cdot \frac{\pi}{4} \left( \frac{d}{f} \right)^2 \cos^4 \alpha
\end{equation}

This effect, known as \textbf{vignetting}, causes images to be naturally brighter at the centre and darker at the periphery.

The light $f(x,y)$ leaving an object is the product of the illumination $i(x,y)$ and the object's \textit{surface reflectance} $r(x,y)$:

\begin{equation}
    f(x,y) = i(x,y) \cdot r(x,y) \quad \text{with } 0 < r(x,y) < 1
\end{equation}

Surface reflectance depends on the properties of the surface, such as colour or micro-geometry.
Depending on the characteristics of the surface, there might be ideal specular objects that act like perfect mirrors, ideal diffusive (Lambertian) objects, or an intermediate case.

\subsubsection{Examples of intensities and reflectance}

\begin{table}[htbp]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Typical illumination intensities.}
        \label{tab:light_sources}
        \begin{tabular}{lc}
            \toprule
            \textbf{Light Source} & \textbf{Intensity $i$ [\unit{\candela}]} \\
            \midrule
            Bright sunny day & \num{10000} \\
            Cloudy day & \num{1000} \\
            Office & \num{100} \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \hfill 
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Typical surface reflectance values.}
        \label{tab:surface_reflectances}
        \begin{tabular}{lc}
            \toprule
            \textbf{Object} & \textbf{Reflectance $r$} \\
            \midrule
            Black velvet & 0.01 \\
            White wall & 0.80 \\
            Snow & 0.93 \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{immagini/reflectance.png}
    \caption{Types of reflectance.}
    \label{fig:reflectance}
\end{figure}

\section{Mathematical Model of Image Acquisition}

An ideal acquisition device transforms an input scene $f(\xi, \eta)$ into an output image $g(x,y)$ via the relation:

\begin{equation}
    g(x,y) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} h(x,y, \xi, \eta) \cdot f(\xi, \eta) \, d\xi \, d\eta
\end{equation}

where $h$ is the \textbf{response function} \textcolor{gray}{( \textbf{point spread function (PSF)} or impulse response)} of the system.
It defines how each point in the object space ($\xi, \eta$) contributes to the formation of the point in the image space ($x,y$), describing the distorsions introduced by the system.
If the system is \textbf{linear} and \textbf{shift-invariant} \textcolor{gray}{(LSI)}, the response depends only on the coordinate difference, becoming a convolution:

\begin{equation}
    g(x,y) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} h(x - \xi, y - \eta) \cdot f(\xi, \eta) \, d\xi \, d\eta
\end{equation}

\subsection{Noise}

Real signals contain \textbf{noise}, defined as random uncertainty. 
In image sequences, statistical noise due to variations in the amount of radiation manifests as intensity fluctuations.
It is typically modelled as an additive Gaussian component $n(x,y)$:

\begin{align}
    g(x,y) & = (h * f)(x,y) + n(x,y) \\
    & = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} h(x - \xi, y - \eta) \cdot f(\xi, \eta) \, d\xi \, d\eta + n(x,y)
\end{align}

Non-statistical noise sources are introduced by the acquisition device, such as \textit{electronic noise}, interferences, crosstalk \textcolor{gray}{ or photon counting statistics (shot noise) and electronic thermal noise}. 
Low photon counts result in lower statistics and higher relative statistical noise.

\section{Digital Images}

\subsection{The digitisation process}

A \textbf{digital image} is a numerical representation of a physical image obtained through \textbf{digitisation}, which comprises two steps:

\begin{enumerate}
    \item \textbf{sampling:} measurement of the grey level of an image at each point.
    \item \textbf{quantisation:} discretisation of amplitude values.
\end{enumerate}

The image is divided into discrete elements called \textbf{pixels} (picture elements). 
Each pixel is described by two numbers representing the position inside the image (row and column) and a value (grey level) representing brightness.
The dynamic range is determined by the minimum and maximum measurable intensities of the device.
A digital image is therefore represented by a 2D matrix of numbers that defines the grey level of each pixel.

\subsection{Definition of digital image}

Formally, a digital image is a sampled, quantised 2D function defined on a rectangular grid.
It is characterised by:

\begin{itemize}
    \item \textbf{spatial resolution:} sampling density, often measured in DPI (dots per inch) or lp/mm (lines per millimetre).
    \item \textbf{grey-level resolution:} the number of quantisation levels, usually a power of 2 (e.g., 256 levels = 8 bits = 1 byte).
\end{itemize}

\subsection{Spatial sampling}

Mathematically, spatial sampling is the multiplication of the continuous function $f(x,y)$ by a "comb" of Dirac delta functions $s(x,y)$:

\begin{equation}
    s(x,y) = \sum_{i=-\infty}^{+\infty} \sum_{j=-\infty}^{+\infty} \delta(x - i\Delta x, y - j\Delta y)
\end{equation}

This extracts values only at the grid nodes, so that it is not necessary to consider averaged values.

The sampled image $f_c(x,y)$ can be written as:

\begin{align}
    f_c(x,y) & = s(x,y) \cdot f(x,y) = \left\{ \sum_{i=-\infty}^{+\infty} \sum_{j=-\infty}^{+\infty} \delta (x - i \Delta x, y - j \Delta y )\right\} \cdot f(x,y) \\
    & = \sum_{i=-\infty}^{+\infty} \sum_{j=-\infty}^{+\infty} f(i \Delta x, j \Delta y) \cdot \delta (x - i \Delta x, y - j \Delta y)
\end{align}

\subsection{Nyquist rate and aliasing} 

To enable proper reconstruction, the sampling interval must satisfy the \textbf{Nyquist rate criterion}: the sampling frequency must be at least twice the highest spatial frequency in the image (or pixel size $\le$ half the size of the smallest resolvable detail).
Failing this leads to \textbf{aliasing}, where high-frequency patterns are misinterpreted as lower frequencies (e.g., MoirÃ© patterns).
The same criterion can be applied to choose the number of grey levels.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{immagini/aliasing.png}
    \caption{Aliasing frequency.}
    \label{fig:aliasing}
\end{figure}

\subsection{Grey-level quantisation}

The continuous intensity is mapped to $L = 2^k$ discrete levels.
The quantisation step should be smaller than half the smallest distinguishable contrast to avoid aliasing \textcolor{gray}{(\textbf{false contouring})} (visible bands in smooth gradients).
The quantisation step introduces a small \textit{quantisation error} in the sample.
The range of values spanned by the grey levels is called \textbf{dynamic range}.

\subsection{Resolution trade-offs}

Higher resolution captures more detail but increases storage and processing costs and requires compatible display hardware. 
Furthermore, extremely high spatial resolution can decrease the Signal-to-Noise Ratio (SNR) per pixel, as smaller pixels collect fewer photons and are more affected by statistical noise.

\subsection{Digital image representation}

A digital image is typically represented as an $M \times N$ matrix of integer numbers ranging from 0 to $2^m - 1$.
For algebraic operations, it is useful to flatten the matrix into a 1D \textbf{lexicographic vector} $\vect{f}$:

\begin{equation}
    \vect{f}(x,y) = \begin{bmatrix}
        f(0,0) \\
        \vdots \\
        f(M-1, 0) \\
        f(0,1) \\
        \vdots \\
        f(M-1, N-1)
    \end{bmatrix}
\end{equation}

Linear operations (like convolution) can then be expressed as matrix-vector multiplication. 
If $\matr{H}$ is the system matrix \textcolor{gray}{(from the PSF)} and $\vect{n}$ is noise, the acquired image can be described as a linear combination of the original image:

\begin{equation}
    \vect{g} = \matr{H} \vect{f} + \vect{n}
\end{equation}

\subsection{Image quality}

Quality assessment is crucial for validating processing algorithms.

\begin{itemize}
    \item \textbf{Subjective assessment:} relies on human observers. It is the ultimate benchmark but is slow, expensive, and variable.
    \item \textbf{Objective assessment:} uses mathematical metrics, such as the normalised mean squared error, to compare the image against a "phantom" (ideal reference) or ground truth, quantifying distortions numerically.
\end{itemize}

The main sources of image degradation include noise, optical distorsions, blurring, and artifacts.

\subsection{Colour images}

Colour images are typically represented as a stack of three matrices (channels), corresponding to the primary colours (e.g., red, green, blue). 
Each channel is processed as an individual grey-level image.